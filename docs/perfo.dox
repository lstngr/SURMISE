/** @page page_perfo Performance

In this section, we use the cluster test case generated from the MATLAB file
"gen_init" (test case 4).

@section scalings-basic General (Strong) Scaling

Ran on an Intel i5-8250U at 1.60GHz. Timing with the UNIX `time` command.
We run for 200 iterations with 1e5 particles and write the output at each
iteration.

| N CPU |  Real  |  User  | System  | SpeedUp |
| ----: | :----: | :----: | :-----: | :-----: |
| 1     | 40.07s | 25.71s | 14.13s  | 1.000   |
| 2     | 34.06s | 53.46s | 14.29s  | 1.176   |
| 3     | 32.77s | 83.37s | 14.07s  | 1.223   |
| 4     | 38.28s | 134.3s | 17.03s  | 1.047   |

The scaling is almost inexistant. It appears that the communication time becomes
too important with already 4CPUs.
We suppose the output writing functions are responsible for this (high system
time), since CPU0 writes while other processes need to wait, and retry with no
outputs.

| N CPU | Real   | User   | System  | SpeedUp |
| ----: | :----: | :----: | :-----: | :-----: |
| 1     | 15.80s | 15.52s | 0.133s  | 1.000   |
| 2     | 9.036s | 17.70s | 0.159s  | 1.749   |
| 3     | 8.398s | 24.54s | 0.288s  | 1.881   |
| 4     | 7.719s | 30.04s | 0.280s  | 2.047   |

Now, we can measure a real speed-up. The strong scaling however seems to be
capped a 4CPUs. This is forcibly due to inefficient use of the MPI library
(remember we use mostly global communication). One may refer to the \link
python_timings timing investigation \endlink for more details.

@section scalings_deneb Scalings with More Cores

Strong scaling up to 64 cores. We use a uniform and clustered sets of 1e6
particles each. The execution time is again measured with the `time` command.
The reported times are the ones from the user portion (the only relevant one,
since the rest of the execution is managed by the SLURM).
Ten iterations of the main loop were executed each time.

| CPUs | Nodes | Uniform | Speedup | Clustered | Speedup |
| :-:  | :-:   | :-:     | :-:     | :-:       | :-:     |
| 1    | 1     | 197.2s  | 1.000   | 83.74s    | 1.000   |
| 2    | 1     | 115.6s  | 1.706   | 63.63s    | 1.316   |
| 4    | 1     | 72.73s  | 2.711   | 56.56s    | 1.480   |
| 8    | 1     | 58.50s  | 3.371   | 52.52s    | 1.594   |
| 16   | 1     | 48.09s  | 4.101   | 49.51s    | 1.691   |
| 32   | 2     | 49.77s  | 3.962   | 50.82s    | 1.648   |
| 64   | 4     |         |         | 57.39s    | 1.459   |

\image html strong.png

Weak scaling for the problem of uniformly distributed masses (cluster set is not
shuffled, so we'd suppress one globe by selecting fewer particles in the
configuration file).
We run with 8 CPUs and 10 up to 1e6 particles (logarithmically increasing), and
theta=0.4.

| Particles | Total Time | Time per CPU |
| :-:       | :-:        | :-:          |
| 10        | 2.366s     | 0.295s       |
| 30        | 2.082s     | 0.260s       |
| 100       | 2.149s     | 0.268s       |
| 300       | 2.295s     | 0.286s       |
| 1000      | 3.175s     | 0.396s       |
| 3000      | 2.542s     | 0.317s       |
| 10000     | 2.528s     | 0.316s       |
| 30000     | 3.357s     | 0.419s       |
| 100000    | 6.703s     | 0.837s       |
| 300000    | 17.382s    | 2.172s       |
| 1000000   | 58.501s    | 7.312s       |

\image html weak.png

@section python_timings Time Distribution

Below, we plot the timings when the program outputs every five iterations and
runs with a large (1e6) number of particles. We clearly see that the workload on
CPU0 (which writes) is the highest. The second most important contribution
(overall) comes from the computation of the forces. (Note T_DISTR, the time to
distribute particles across MPI instances is constant since only read before the
main iteration).
\image html tpercpu.png
\image html tpertask.png
\image html tperiter.png

Unfortunately, the time with less calls to the IO routines is not relevant due
to a failing implementation of the Timer class to handle multiple time steps.

@section valgrind Memory leaks

The application is checked for memory leaks using Valgrind 3.14.0. It is found
that, unless a crash occurs, the memory is freed entirely at the end of a run.
*/
