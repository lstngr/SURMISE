/** @page page_perfo Performance

In this section, we use the cluster test case generated from the MATLAB file
"gen_init" (test case 4).

@section scalings-basic General (Strong) Scaling

Ran on an Intel i5-8250U at 1.60GHz. Timing with the UNIX `time` command.
We run for 200 iterations with 1e5 particles and write the output at each
iteration.

| N CPU |  Real  |  User  | System  | SpeedUp |
| ----: | :----: | :----: | :-----: | :-----: |
| 1     | 40.07s | 25.71s | 14.13s  | 1.000   |
| 2     | 34.06s | 53.46s | 14.29s  | 1.176   |
| 3     | 32.77s | 83.37s | 14.07s  | 1.223   |
| 4     | 38.28s | 134.3s | 17.03s  | 1.047   |

The scaling is almost inexistant. It appears that the communication time becomes
too important with already 4CPUs.
We suppose the output writing functions are responsible for this (high system
time), since CPU0 writes while other processes need to wait, and retry with no
outputs.

| N CPU |  Real  |  User  | System  | SpeedUp |
| ----: | :----: | :----: | :-----: | :-----: |
| 1     | 15.80s | 15.52s | 0.133s  | 1.000   |
| 2     | 9.036s | 17.70s | 0.159s  | 1.749   |
| 3     | 8.398s | 24.54s | 0.288s  | 1.881   |
| 4     | 7.719s | 30.04s | 0.280s  | 2.047   |

Now, we can measure a real speed-up. The strong scaling however seems to be
capped a 4CPUs. This is forcibly due to inefficient use of the MPI library
(remember we use mostly global communication). One may refer to the \link
python_timings timing investigation \endlink for more details.

@section scalings_deneb Scalings with More Cores

Some runs indicate similar scalings as before without output to files, but due
to frequent crashes that I fail to diagnose, this section is left empty.

@section python_timings Time Distribution

Below, we plot the timings when the program outputs every iteration. We clearly
see that the workload on CPU0 (which writes) is the highest. The second most
important contribution (overall) comes from the computation of the forces. (Note
T_DISTR, the time to distribute particles across MPI instances is constant since
only read before the main iteration).
\image html tpercpu.png
\image html tpertask.png
\image html tperiter.png

Unfortunately, the time with less calls to the IO routines is not relevant due
to a failing implementation of the Timer class to handle multiple time steps.

@section valgrind Memory leaks

The application is checked for memory leaks using Valgrind 3.14.0. It is found
that, unless a crash occurs, the memory is freed entirely at the end of a run.
*/
